**Day0: [Lab2-0&1 Tokenization](https://colab.research.google.com/drive/1mGCQe-dGidYaLxXZbQ92RaxkI2WfiWKv?usp=sharing), [Lab2-2 LSTM](https://colab.research.google.com/drive/1B6sFWxloF93nmiTWOJkkp-unYIuDyScX?usp=sharing)**   
Today we will wrap up RNN discussions  
[BERT Preprocessing](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)  
[Classifying text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)  

*Optional Readings*  
[Karpathy, A., Johnson, J., & Fei-Fei, L. (2015). Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf)  
[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://arxiv.org/abs/1706.03762) 


**Day1: [CNN], [Lab2-3 MNIST101], [Lab2-4 MNIST with CNN]**  
