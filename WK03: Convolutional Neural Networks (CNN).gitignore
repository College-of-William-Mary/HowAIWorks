**Day 0: [Lab2-0&1 Tokenization](https://colab.research.google.com/drive/1mGCQe-dGidYaLxXZbQ92RaxkI2WfiWKv?usp=sharing), [Lab2-2 LSTM](https://colab.research.google.com/drive/1B6sFWxloF93nmiTWOJkkp-unYIuDyScX?usp=sharing)**   
Today we will wrap up RNN discussions  
[Regularization: Playground Exercise](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/playground-exercise)  
[BERT Preprocessing](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)  
[Classifying text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)  
[ImageNet classification with deep convolutional neural networks](https://dl.acm.org/doi/abs/10.1145/3065386)  

**Complete these readings/videos before next class**  
[Chapter 10.3 Convolutional Neural NetworkNetworks, ISLR2](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)  
[RNN with Keras](https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/rnn.ipynb)  
[Fei-Fei Li's TED Talk: How we teach computers to understand pictures](https://youtu.be/40riCqvRoMs)  

[Explore image datasets with Know your Data from Google](https://knowyourdata.withgoogle.com/)   
[Yann Lecun's MNIST site](http://yann.lecun.com/exdb/mnist/)  
[Yann Lecun's TED Talk on Deep learning, neural networks and the future of AI](https://www.ted.com/talks/yann_lecun_deep_learning_neural_networks_and_the_future_of_ai)  
[MNIST digit classification dataset by Keras](https://keras.io/api/datasets/mnist/)  
[Tensorflow2 quickstart for beginners](https://www.tensorflow.org/tutorials/quickstart/beginner)  

*Optional Fun Stuff*  
[Backpropagation & Optimization by CrashCourse AI](https://www.pbs.org/video/training-neural-networks-4-mq025r/)  
[Teachable Machine](https://teachablemachine.withgoogle.com/)  

*References*  
[Karpathy, A., Johnson, J., & Fei-Fei, L. (2015). Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.](http://vision.stanford.edu/pdf/KarpathyICLR2016.pdf)  
[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://arxiv.org/abs/1706.03762) 

**Day 1: [CNN](https://www.dropbox.com/s/mbbjfmm0bkzqenz/03-0%20CNN.pptx?dl=0), [Lab2-3 MNIST101](https://colab.research.google.com/drive/1tpnpL5ulGroOluAFkLZCDpt0e9PH3iWF?usp=sharing), [Lab2-4 MNIST with CNN](https://colab.research.google.com/drive/1546EfPVvdJGrKz9rQaoJtZGjcqsL46GW?usp=sharing)**  

**Complete these readings/videos before next class**  
[Dr. Chung's TF Keras CNN Model Template](https://docs.google.com/document/d/1gw1SanV6caqE4-iZAn3TO3tZ4poCYL3yvPZbidMqCxw/edit?usp=sharing)  
[Keras Flatten Layer](https://keras.io/api/layers/reshaping_layers/flatten/)  
[CNN Tutorial by Tensorflow](https://www.tensorflow.org/tutorials/images/cnn)  
[Basic image operations with OpenCV](https://docs.opencv.org/3.4/d3/df2/tutorial_py_basic_ops.html)  

*Optional Fun Stuff*  
[Cats vs. Dogs? Lets Make an AI to Settle This by CrashCourse AI](https://www.pbs.org/video/cats-vs-dogs-lets-make-an-ai-to-settle-this-lab-19-rp1lwa/)  
